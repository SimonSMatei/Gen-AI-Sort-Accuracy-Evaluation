# Project Overview
This research project evaluates the zero-shot reasoning capabilities of various Large Language Models (LLMs) when tasked with basic algorithmic operationsâ€”specifically, sorting numerical lists. While Generative AI excels at semantic tasks, its ability to perform strict logical operations on token sequences remains a subject of active research. This project quantifies the error rate of models (specific models to be decided) as input complexity increases.

### Objectives
* **Accuracy Benchmarking:** Measure the sort accuracy of different models across varying list lengths (n=10, 100, 500) and the sort accuracy of sorting a list with guaranteed duplicate numbers.
* **Hallucination Detection:** Identify specific failure modes, such as dropped numbers and hallucinated values.

### Methodology
* **Data Generation:** A random list of integers (1-10 for duplicate list and 1-10,000 for the rest) with specified lengths are generated by ***Get_Lists.py*** and stored in respective files.
* **Model Inference:** Lists are passed to model APIs via a standardized prompt template requesting a sorted output.
* **Validation:** The response is then compared to the true sorted list and the errors are stored in respective files.
* **Modeling and Statistics:** Errors are passed through a script to visualize and gather wanted statistical analysis.

### Tech Stack
* **Development Environment:** Google Antigravity (Agentic IDE)
* **Language:** Python 3.13.2
* **Models Tested:** TBD
* **Libraries:** pandas, matplotlib

---

**Disclaimer:** This project is still in the early stages so many items are subject to change. This overview provides a plan of action, not all parts have been completed.
